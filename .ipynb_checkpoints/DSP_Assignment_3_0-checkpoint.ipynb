{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <h1>Assignment #3</h1> </center>\n",
    "<center> <h2>DSP</h2> </center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.io import imread, imshow\n",
    "from skimage.filters import prewitt_h,prewitt_v\n",
    "import os\n",
    "from os import listdir\n",
    "import numpy as np \n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd    \n",
    "from scipy.fft import dct\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.stats import mode\n",
    "from scipy.fft import dct\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "from sklearn import mixture\n",
    "from sklearn import svm\n",
    "from skimage.feature import hog\n",
    "from skimage import data, exposure\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "import keras \n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, Dense, MaxPool2D, Dropout, Flatten,AvaragePooling2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "#%%\n",
    "x_tst=[]\n",
    "y_tst=[]                      \n",
    "TST_PATH = r'E:\\engineering\\Learning\\EECE\\fourth year\\second term\\dsp\\projects\\assignment2\\Reduced MNIST Data\\Reduced Testing data'\n",
    "for digit in listdir(TST_PATH):\n",
    "    for ex in listdir(TST_PATH+'\\\\'+digit):\n",
    "        x_tst.append(imread(TST_PATH+'\\\\'+digit+'\\\\'+ex))\n",
    "        y_tst.append(digit)\n",
    "x_tst=np.asarray(x_tst)\n",
    "y_tst=np.asarray(y_tst)\n",
    "\n",
    "x_train=[]\n",
    "y_train=[]\n",
    "TRAIN_PATH = r'E:\\engineering\\Learning\\EECE\\fourth year\\second term\\dsp\\projects\\assignment2\\Reduced MNIST Data\\Reduced Trainging data'\n",
    "for digit in listdir(TRAIN_PATH):\n",
    "    for ex in listdir(TRAIN_PATH+'\\\\'+digit):\n",
    "        x_train.append(imread(TRAIN_PATH+'\\\\'+digit+'\\\\'+ex))\n",
    "        y_train.append(digit)\n",
    "#x_train=np.asarray(x_train)\n",
    "#y_train=np.asarray(y_train)\n",
    "#print(x_train.shape,y_train.shape) \n",
    "#print(x_tst.shape,y_tst.shape)\n",
    "\n",
    "# train=pd.DataFrame({'img':x_train,'label':y_train})\n",
    "# train=train.sample(frac=1).reset_index(drop=True)\n",
    "# x_train=np.stack(np.asarray(train['img']))\n",
    "# y_train=np.asarray(train['label'])\n",
    "\n",
    "x_train=np.asarray(x_train).reshape((10000,28,28))\n",
    "print(x_train.shape) \n",
    "x_tst=np.asarray(x_tst).reshape((2000,28,28))\n",
    "print(x_tst.shape)\n",
    "y_train=np.asarray(y_train).astype(int)\n",
    "y_tst=np.asarray(y_tst).astype(int)\n",
    "\n",
    "#%%\n",
    "\n",
    "\n",
    "\"\"\" DCT Feature extraction\"\"\"\n",
    "\n",
    "dct_train_data = dct(dct(x_train, axis=1), axis=2)\n",
    "dct_tst_data = dct(dct(x_tst, axis=1), axis=2)\n",
    "print(dct_train_data.shape)\n",
    "print(dct_tst_data.shape)\n",
    "\n",
    "\n",
    "\n",
    "rows=28\n",
    "columns=28\n",
    "dct_train_data_compressed=np.empty((10000,180))\n",
    "solution=[]\n",
    "for k in range(10000):    \n",
    "    solution=[]\n",
    "    for i in range(rows):\n",
    "        for j in range(columns):\n",
    "            if((i+j) < 19):\n",
    "                solution.append(dct_train_data[k, i, j])\n",
    "    dct_train_data_compressed[k]=np.asarray(solution[:-10])\n",
    "    \n",
    "print(dct_train_data_compressed.shape)\n",
    "\n",
    "dct_tst_data_compressed=np.empty((2000,180))\n",
    "for k in range(2000):    \n",
    "    solution=[]\n",
    "    for i in range(rows):\n",
    "        for j in range(columns):\n",
    "            if((i+j) < 19):\n",
    "                solution.append(dct_tst_data[k, i, j])\n",
    "    dct_tst_data_compressed[k]=np.asarray(solution[:-10])\n",
    "print(dct_tst_data_compressed.shape)\n",
    "x_train_dct=dct_train_data_compressed\n",
    "x_tst_dct=dct_tst_data_compressed\n",
    "\n",
    "\"\"\" PCA Feature extraction\"\"\"\n",
    "pca_breast = PCA(n_components=270)\n",
    "x_train_pca = pca_breast.fit_transform(x_train.reshape((10000,784)))\n",
    "x_tst_pca = pca_breast.transform(x_tst.reshape((2000,784)))\n",
    "sum(pca_breast.explained_variance_ratio_)\n",
    "\n",
    "#%%\n",
    "def HOG_(images):\n",
    "    feature_array=[]\n",
    "    for image in images:\n",
    "        ret, bw_img = cv2.threshold(image,127,255,cv2.THRESH_BINARY)\n",
    "        features, hog_img = hog(bw_img, orientations=8, pixels_per_cell=(6, 6), visualize=True, multichannel=False)\n",
    "       # features = exposure.rescale_intensity(features, in_range=(0, 10))\n",
    "        #features=np.where(features.reshape([28*28,])==1)[0]\n",
    "        feature_array.append(features)\n",
    "    \n",
    "    # list_len = [len(i) for i in feature_array]\n",
    "    # max_len=(max(list_len))\n",
    "    \n",
    "    # for i,ex in enumerate(feature_array):\n",
    "    #    feature_array[i]=np.append(ex,np.zeros(max_len-len(ex)))\n",
    "   \n",
    "    feature_array=np.asarray(feature_array)\n",
    "\n",
    "    return feature_array\n",
    "x_tst_edge=HOG_(x_tst)\n",
    "x_train_edge=HOG_(x_train)\n",
    "\n",
    "#%%\n",
    "\n",
    "def KM(X,Y,Xt,Yt,n):\n",
    "    \n",
    "    t = time.time()\n",
    "\n",
    "    kmeans = KMeans(n_clusters=n, random_state=0).fit(X)\n",
    "    t_elapsed=time.time()-t\n",
    "    classes=kmeans.labels_\n",
    "    \n",
    "    # print([sum(classes==i) for i in range(n)])\n",
    "    # print(sum([sum(classes==i) for i in range(n)]))\n",
    "    \n",
    "    \n",
    "    \n",
    "    labels = np.zeros_like(classes)\n",
    "    translation=np.zeros((n,1))\n",
    "    for i in range(n):\n",
    "        mask = (classes == i)\n",
    "        labels[mask] = mode(Y[mask])[0]\n",
    "        translation[i]=mode(Y[mask])[0]\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    prediction =kmeans.predict(Xt)\n",
    "    for i in range(len(prediction)):\n",
    "        prediction[i]=translation[prediction[i]]\n",
    "    \n",
    "    \n",
    "    accuarcy=accuracy_score(prediction, Yt)\n",
    "    train_accuarcy=accuracy_score(labels, Y)\n",
    "    \n",
    "    return train_accuarcy,accuarcy,t_elapsed\n",
    "    \n",
    " #%%\n",
    "[acc_KM_10_pca_train,acc_KM_10_pca_tst,time_KM_10_pca_train]=KM(x_train_pca,y_train,x_tst_pca,y_tst,10) \n",
    "[acc_KM_40_pca_train,acc_KM_40_pca_tst,time_KM_40_pca_train]=KM(x_train_pca,y_train,x_tst_pca,y_tst,40)\n",
    "[acc_KM_160_pca_train,acc_KM_160_pca_tst,time_KM_160_pca_train]=KM(x_train_pca,y_train,x_tst_pca,y_tst,160)\n",
    "\n",
    "    \n",
    "#%%\n",
    "[acc_KM_10_dct_train,acc_KM_10_dct_tst,time_KM_10_dct_train]=KM(x_train_dct,y_train,x_tst_dct,y_tst,10) \n",
    "[acc_KM_40_dct_train,acc_KM_40_dct_tst,time_KM_40_dct_train]=KM(x_train_dct,y_train,x_tst_dct,y_tst,40)\n",
    "[acc_KM_160_dct_train,acc_KM_160_dct_tst,time_KM_160_dct_train]=KM(x_train_dct,y_train,x_tst_dct,y_tst,160)\n",
    "\n",
    "#%%\n",
    "\n",
    "[acc_KM_10_edge_train,acc_KM_10_edge_tst,time_KM_10_edge_train]=KM(x_train_edge,y_train,x_tst_edge,y_tst,10) \n",
    "[acc_KM_40_edge_train,acc_KM_40_edge_tst,time_KM_40_edge_train]=KM(x_train_edge,y_train,x_tst_edge,y_tst,40) \n",
    "[acc_KM_160_edge_train,acc_KM_160_edge_tst,time_KM_160_edge_train]=KM(x_train_edge,y_train,x_tst_edge,y_tst,160) \n",
    "\n",
    "#%%\n",
    "print('KM 10')\n",
    "print('[pca_train, pca_tst,Time_elapsed]',[acc_KM_10_pca_train,acc_KM_10_pca_tst,time_KM_10_pca_train])\n",
    "print('[dct_train, dct_tst,Time_elapsed]',[acc_KM_10_dct_train,acc_KM_10_dct_tst,time_KM_40_pca_train])\n",
    "print('[hog_train, hog_tst,Time_elapsed]',[acc_KM_10_edge_train,acc_KM_10_edge_tst,time_KM_160_pca_train])\n",
    "\n",
    "print('KM 40')\n",
    "print('[pca_train, pca_tst,Time_elapsed]',[acc_KM_40_pca_train,acc_KM_40_pca_tst,time_KM_10_dct_train])\n",
    "print('[dct_train, dct_tst,Time_elapsed]',[acc_KM_40_dct_train,acc_KM_40_dct_tst,time_KM_40_dct_train])\n",
    "print('[hog_train, hog_tst,Time_elapsed]',[acc_KM_40_edge_train,acc_KM_40_edge_tst,time_KM_160_dct_train])\n",
    "\n",
    "print('KM 160')\n",
    "print('[pca_train, pca_tst,Time_elapsed]',[acc_KM_160_pca_train,acc_KM_160_pca_tst,time_KM_10_edge_train])\n",
    "print('[dct_train, dct_tst,Time_elapsed]',[acc_KM_160_dct_train,acc_KM_160_dct_tst,time_KM_40_edge_train])\n",
    "print('[hog_train, hog_tst,Time_elapsed]',[acc_KM_160_edge_train,acc_KM_160_edge_tst,time_KM_160_edge_train])\n",
    "\n",
    "\n",
    "#%%\n",
    "def GM(X,Y,Xt,Yt,n):\n",
    "    \n",
    "    t = time.time()\n",
    "\n",
    "    gmm = mixture.GaussianMixture(n_components=n).fit(X)\n",
    "    t_elapsed=time.time()-t\n",
    "\n",
    "    classes=gmm.predict(X)\n",
    "    \n",
    "    # print([sum(classes==i) for i in range(n)])\n",
    "    # print(sum([sum(classes==i) for i in range(n)]))\n",
    "    \n",
    "    \n",
    "    \n",
    "    labels = np.zeros_like(classes)\n",
    "    translation=np.zeros((n,1))\n",
    "    for i in range(n):\n",
    "        mask = (classes == i)\n",
    "        labels[mask] = mode(Y[mask])[0]\n",
    "        translation[i]=mode(Y[mask])[0]\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    prediction =gmm.predict(Xt)\n",
    "    for i in range(len(prediction)):\n",
    "        prediction[i]=translation[prediction[i]]\n",
    "    \n",
    "    \n",
    "    accuarcy=accuracy_score(prediction, Yt)\n",
    "    train_accuarcy=accuracy_score(labels, Y)\n",
    "    \n",
    "    return train_accuarcy,accuarcy,t_elapsed\n",
    "#%%\n",
    "[acc_GMM_10_pca_train,acc_GMM_10_pca_tst,time_GMM_10_pca_train]=GM(x_train_pca,y_train,x_tst_pca,y_tst,10) \n",
    "[acc_GMM_40_pca_train,acc_GMM_40_pca_tst,time_GMM_40_pca_train]=GM(x_train_pca,y_train,x_tst_pca,y_tst,40)\n",
    "[acc_GMM_160_pca_train,acc_GMM_160_pca_tst,time_GMM_160_pca_train]=GM(x_train_pca,y_train,x_tst_pca,y_tst,160)\n",
    "\n",
    "    \n",
    "#%%\n",
    "\n",
    "[acc_GMM_10_dct_train,acc_GMM_10_dct_tst,time_GMM_10_dct_train]=GM(x_train_dct,y_train,x_tst_dct,y_tst,10) \n",
    "[acc_GMM_40_dct_train,acc_GMM_40_dct_tst,time_GMM_40_dct_train]=GM(x_train_dct,y_train,x_tst_dct,y_tst,40)\n",
    "[acc_GMM_160_dct_train,acc_GMM_160_dct_tst,time_GMM_160_dct_train]=GM(x_train_dct,y_train,x_tst_dct,y_tst,160)\n",
    "\n",
    "#%%\n",
    "\n",
    "[acc_GMM_10_edge_train,acc_GMM_10_edge_tst,time_GMM_10_edge_train]=GM(x_train_edge,y_train,x_tst_edge,y_tst,10)\n",
    "[acc_GMM_40_edge_train,acc_GMM_40_edge_tst,time_GMM_40_edge_train]=GM(x_train_edge,y_train,x_tst_edge,y_tst,40) \n",
    "[acc_GMM_160_edge_train,acc_GMM_160_edge_tst,time_GMM_160_edge_train]=GM(x_train_edge,y_train,x_tst_edge,y_tst,160) \n",
    "#%%\n",
    "print('GMM 10')\n",
    "print('[pca_train, pca_tst,Time_elapsed]',[acc_GMM_10_pca_train,acc_GMM_10_pca_tst,time_GMM_10_pca_train])\n",
    "print('[dct_train, dct_tst,Time_elapsed]',[acc_GMM_10_dct_train,acc_GMM_10_dct_tst,time_GMM_40_pca_train])\n",
    "print('[hog_train, hog_tst,Time_elapsed]',[acc_GMM_10_edge_train,acc_GMM_10_edge_tst,time_GMM_160_pca_train])\n",
    "\n",
    "print('GMM 40')\n",
    "print('[pca_train, pca_tst,Time_elapsed]',[acc_GMM_40_pca_train,acc_GMM_40_pca_tst,time_GMM_10_dct_train])\n",
    "print('[dct_train, dct_tst,Time_elapsed]',[acc_GMM_40_dct_train,acc_GMM_40_dct_tst,time_GMM_40_dct_train])\n",
    "print('[hog_train, hog_tst,Time_elapsed]',[acc_GMM_40_edge_train,acc_GMM_40_edge_tst,time_GMM_160_dct_train])\n",
    "\n",
    "print('GMM 160')\n",
    "print('[pca_train, pca_tst,Time_elapsed]',[acc_GMM_160_pca_train,acc_GMM_160_pca_tst,time_GMM_10_edge_train])\n",
    "print('[dct_train, dct_tst,Time_elapsed]',[acc_GMM_160_dct_train,acc_GMM_160_dct_tst,time_GMM_40_edge_train])\n",
    "print('[hog_train, hog_tst,Time_elapsed]',[acc_GMM_160_edge_train,acc_GMM_160_edge_tst,time_GMM_160_edge_train])\n",
    "\n",
    "\n",
    "\n",
    "#%%\n",
    "def SVM(X,Y,Xt,Yt,k):\n",
    "\n",
    "    clf = svm.SVC(kernel=k) # Linear Kernel\n",
    "    t = time.time()\n",
    "    clf.fit(X, Y)\n",
    "    t_elapsed=time.time()-t\n",
    "\n",
    "    labels = clf.predict(X)\n",
    "    y_pred = clf.predict(Xt)\n",
    "        \n",
    "    train_accuarcy = accuracy_score(Y, labels)\n",
    "    accuarcy = accuracy_score(Yt, y_pred)\n",
    "\n",
    "    return train_accuarcy,accuarcy,t_elapsed\n",
    "\n",
    "#%%\n",
    "[acc_SVM_pca_train,acc_SVM_pca_tst,time_SVM_pca_train]=SVM(x_train_pca,y_train,x_tst_pca,y_tst,'Linear') \n",
    "[acc_SVM_dct_train,acc_SVM_dct_tst,time_SVM_dct_train]=SVM(x_train_dct,y_train,x_tst_dct,y_tst,'Linear') \n",
    "[acc_SVM_edge_train,acc_SVM_edge_tst,time_SVM_edge_train]=SVM(x_train_edge,y_train,x_tst_edge,y_tst,'Linear') \n",
    "print('SVM_Linear')\n",
    "print('[pca_train, pca_tst , time_elapsed]',[acc_SVM_pca_train,acc_SVM_pca_tst,time_SVM_pca_train])\n",
    "print('[dct_train, dct_tst , time_elapsed]',[acc_SVM_dct_train,acc_SVM_dct_tst,time_SVM_dct_train])\n",
    "print('[hog_train, hog_tst , time_elapsed]',[acc_SVM_edge_train,acc_SVM_edge_tst,time_SVM_edge_train])\n",
    "\n",
    "#%%\n",
    "[acc_SVM_pca_train_,acc_SVM_pca_tst_,time_SVM_pca_train_]=SVM(x_train_pca,y_train,x_tst_pca,y_tst,'rbf') \n",
    "[acc_SVM_dct_train_,acc_SVM_dct_tst_,time_SVM_dct_train_]=SVM(x_train_dct,y_train,x_tst_dct,y_tst,'rbf') \n",
    "[acc_SVM_edge_train_,acc_SVM_edge_tst_,time_SVM_edge_train_]=SVM(x_train_edge,y_train,x_tst_edge,y_tst,'rbf') \n",
    "print('SVM_RPF')\n",
    "print('[pca_train, pca_tst , time_elapsed]',[acc_SVM_pca_train_,acc_SVM_pca_tst_,time_SVM_pca_train_])\n",
    "print('[dct_train, dct_tst , time_elapsed]',[acc_SVM_dct_train_,acc_SVM_dct_tst_,time_SVM_dct_train_])\n",
    "print('[hog_train, hog_tst , time_elapsed]',[acc_SVM_edge_train_,acc_SVM_edge_tst_,time_SVM_edge_train_])\n",
    "#%%\n",
    "\n",
    "leNet=tf.keras.models.Sequential()\n",
    "leNet.add(Conv2D(filters=8, kernel_size=(5,5), padding='same', activation='relu', input_shape=(28, 28, 1)))\n",
    "leNet.add(AvaragePooling2D(strides=2))\n",
    "leNet.add(Conv2D(filters=16, kernel_size=(5,5), padding='valid', activation='relu'))\n",
    "leNet.add(AvaragePooling2D(strides=2))\n",
    "leNet.add(Flatten())\n",
    "leNet.add(Dense(256, activation='relu'))\n",
    "leNet.add(Dense(84, activation='relu'))\n",
    "leNet.add(Dense(10, activation='softmax'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
